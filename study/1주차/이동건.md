
# 🧠 Transformer 모델 완전 정복

## 🚀 Transformer란?

**Transformer**는 RNN이나 CNN 없이 **Attention 메커니즘**만으로 시퀀스(문장처럼 순서가 있는 데이터)를 처리하는 딥러닝 모델 구조입니다.

---

## 🔧 Transformer의 기본 구조

Transformer는 크게 두 부분으로 나뉩니다:

```
[ Input Sentence ] → [Encoder Stack] → [Decoder Stack] → [ Output Sentence ]
```

### 🔵 1. Encoder

- 입력 문장을 받는 부분
- 총 6개의 **같은 구조의 Layer**로 이루어짐
- 각 레이어 구성:
  - **Multi-Head Self-Attention**
  - **Feed-Forward Network (FFN)**
  - **Residual Connection + LayerNorm**

### 🟠 2. Decoder

- 번역/생성 결과를 만들어내는 부분
- Encoder와 마찬가지로 6개의 동일한 구조의 레이어
- 각 레이어 구성:
  - **Masked Multi-Head Self-Attention**
  - **Encoder-Decoder Attention**
  - **Feed-Forward Network**
  - **Residual Connection + LayerNorm**

---

## 🎯 핵심 원리: Self-Attention

### 📌 Self-Attention이란?

→ **"문장 내 모든 단어를 서로 비교해, 어떤 단어가 어떤 단어에 집중해야 할지를 계산"**

예:  
```
The animal didn't cross the street because it was too tired.
```
- 여기서 'it'이 누구를 가리키는지 파악할 때 ‘animal’과의 관계에 집중할 필요가 있음

---

### ⚙️ 계산 원리

1. 각 단어 임베딩에서 **Query (Q)**, **Key (K)**, **Value (V)** 벡터 생성
2. Q와 K의 내적 → 유사도 계산
3. Softmax로 유사도 점수를 확률로 변환
4. 각 단어의 Value에 가중합을 적용해 새로운 표현 생성

수식:
```
Attention(Q, K, V) = softmax(QK^T / sqrt(dk)) * V
```

---

## 🤯 Multi-Head Attention

- Attention을 **여러 Head**로 나눠서 각각 다른 관점에서 정보에 집중
- 예: 하나의 Head는 동사에, 다른 Head는 명사에 집중

💡 **비유**: 각각의 Head는 다른 전문가들이 같은 문장을 분석하는 느낌

---

## 🧱 Feed-Forward Network (FFN)

- Attention 결과를 **비선형 변환**으로 가공
- 각 단어 위치마다 **같은 FFN**을 적용
- 수식:  
```
FFN(x) = ReLU(xW1 + b1)W2 + b2
```

---

## 🧭 Positional Encoding

Transformer는 단어 순서를 알 수 없기 때문에 **위치 정보**를 인위적으로 부여합니다.

- **사인/코사인 함수**로 위치를 인코딩:
```
PE(pos, 2i) = sin(pos / 10000^(2i/dmodel))
PE(pos, 2i+1) = cos(pos / 10000^(2i/dmodel))
```

---

## 🧪 Transformer의 장점

| 항목 | 내용 |
|------|------|
| 병렬 연산 가능 | 단어들을 동시에 처리 가능 |
| 긴 거리 의존성 | 멀리 떨어진 단어도 직접 연결 |
| 성능 우수 | 번역, 문서 요약, 생성에서 최고의 성능 |
| 확장성 | GPT, BERT, T5 등으로 발전 가능 |

---

## 📘 그림 비유

```
[단어1]──────▶
     │   ▲
     ▼   │        (Attention: 서로 연결됨)
[단어2]──────▶
     │   ▲
     ▼   │
[단어3]──────▶

→ 모든 단어가 서로를 주시하며 정보 교환
```

---

## 📚 요약

Transformer는 Attention만으로 시퀀스를 처리하는 혁신적인 모델입니다.  
이 구조는 현대 NLP 모델의 표준이 되었고, 앞으로도 그 영향력은 계속될 것입니다.
