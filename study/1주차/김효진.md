# Attention is all you need 논문 리뷰

이전 Seq2seq : 고정된 크기의 context vector 사용.
Transformer : rnn 기반의 아키텍처 사용할 필요 X. attention만 사용.

기존 seq 2 seq 모델의 한계점 : 고정된 크기의 context vector 디코더의 rnn 셀에서 참고해서 문장의 길이에 대한 정보를 추가하지만 여전히 source 문장을 하나의 context vector에 압축해야 한다는 문제점이 있다.

Seq2seq with attention
단어 -> hidden state 값을 매번 가짐. 소스 문장에서 출력된 모든 hidden state 값에 대한 weight sum 을 추출 후 디코더에 적용.

Transformer : attention만 사용. Rnn, cnn안쓴다. 대신 Positional encoding사용. => 문장에서 단어의 순서 정보.

*
인코더, 디코더는 여러 개의 레이어를 갖는다. 각 레이어는 서로 다른 파라미터를 갖지만, 크기는 같다.
인코더, 디코더는 여러 개의 레이어로 이루어져 있으며, 가장 마지막 레이어에서 나온 출력값이 최종 값.

## transformer 동작 원리 – 인코더
문장 소스 : I am a teacher.
Embedding dimension(ex.512) 입력값들을 임베딩 형태로 사용하기 위한 레이어.
Rnn을 사용하면, 자동으로 각각의 hidden state 값이 순서를 가진다. 하지만 attention만으로는 위치에 대한 정보가 없기 때문에 positional encoding을 사용한다. (위치에 대한 정보값)

인코딩 파트의 attention : 각각의 단어가 다른 단어와 어떤 연관성을 갖는지 학습한다. 문맥 추론.

Input Embeddings matrix + Positional encoding(위치 정보)
=> multi-head attention

Residual connection(잔여된 부분만 학습) => add + norm
Attention 수행 값 + residual connection => normalize해서 사용.

## transformer 동작 원리 - 디코더
입력 dimension과 출력 dimension 크기 같음. => 병렬로 여러 번 처리 가능!
1st self attention. 출력되는 문장에 대한 전반적인 표현 학습.
2nd 인코더의 정보를 받아, 각각의 출력 단어가 소스 문장과 어떤 연관성이 있는지 파악.
 
*마지막 인코더 레이어의 출력이 모든 디코더 레이어에 입력된다. ‘Multi-head attention’

Seq2seq같이 context vector를 압축하는 과정이 생략. 입력 문장의 encoding vector를 그대로 받아 학습에 사용할 수 있다.

### Multi-head attention
하나의 attention은 query, key, value를 받고, 어떤 key에 대해 높은 가중치를 갖는지 계산. Dk : 각각의 key dimension. 루트 dk : sccale factor.
H개의 서로 다른 Q, K, V 쌍을 만들 수 있다.

원본 논문에서는 임베딩 차원을 512차원으로 사용.
아래는 임베딩 차원 4차원을 q, k, v는 2차원으로 변환한 것.
에너지 : 소스 문장의 모든 출력값들 중에서 가장 연관성이 높은 것을 찾기 위한 수치를 구한 것.
#### example
쿼리(query) : I
키(key) : I am a teacher
I 라는 단어가 I am a teacher에서 어떤 가중치값을 갖는지.
값(value)
입력으로 들어 온 값이 세 개로 복제되어 각각 value, key, query로 들어가고, 행렬곱으로 h개로 구분된 각각의 value, key, query쌍을 만들어 내보낸다. => concat, linear로 입력값과 출력값의 크기가 같게 만든다.
디코더값이 query, 인코더의 출력값이 value, key가 된다.
 
I love you. Ex. I 라는 단어는 I 와 72 %, love와 15%, you와 13%의 연관성.
